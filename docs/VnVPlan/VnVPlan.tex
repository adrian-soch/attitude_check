\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{enumitem,amssymb}
\usepackage{amsmath, mathtools}
\usepackage{amsfonts}
\usepackage{bm}
\hypersetup{ colorlinks, citecolor=blue, filecolor=black, linkcolor=red, urlcolor=blue }
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}

\newcommand{\testref}[1]{T\ref{#1}} \newcounter{testnum} %Requirement Number
\newcommand{\Tthetestnum}{T\thetestnum}

\newcommand{\utestref}[1]{UT\ref{#1}} \newcounter{utestnum} %Requirement Number
\newcommand{\UTthetestnum}{UT\theutestnum}

\begin{document}

\title{Project Title: System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X} \toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
2024/02/13 & 1.0 & Initial draft\\
\bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software. However, this does not
mean listing every verification and validation technique that has ever been devised.  The VnV plan
should also be a \textbf{feasible} plan. Execution of the plan should be possible with the time and
team available. If the full plan cannot be completed during the time available, it can either be
modified to ``fake it'', or a better solution is to add a section describing what work has been
completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before the design stage.
This means that the sections related to unit testing cannot initially be completed.  The sections
will be filled in after the design stage is complete.  the final version of the VnV plan should have
all sections filled in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}

For symbols and units see the Section 1 of the SRS \citep{SRS}. Table \ref{tab:abb} defines the
abbreviations and acronyms used in this document.

% \renewcommand{\arraystretch}{1.2} \begin{tabular}{l l} \toprule        
%   \textbf{symbol} & \textbf{description}\\
%   \midrule T & Test\\
%   \bottomrule \end{tabular}\\

\begin{table}[!h]
  \centering
  \caption{Table of Abbreviations and Acronyms}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
    \toprule
    \textbf{symbol} & \textbf{description}\\
    \midrule 
    % TM & Theoretical Model\\
    accel & Accerleromter \\
    gyro & Gyroscope \\
    IMU & Inertial Measurement Unit\\
    mag & Magnetometer \\
    SRS & Software Requirements Specification\\
    VnV & Verification and Validation \\
    % MEMS & Micro-electromechanical System \\
    % NED & North-East-Down \\
    % WMM & World Magnetic Model. \\
    \bottomrule
\end{tabular}\\
\label{tab:abb}
\end{table}

\wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS \citep{SRS} tables, if
  appropriate}

\wss{Remove this section if it isn't needed}

\newpage

\pagenumbering{arabic}

\section{Introduction}

A verification and validation (VnV) plan is a document that describes the objectives, scope,
methods, and criteria for verifying and validating a software product. Verification is the process
of checking whether the software meets the specified requirements and design specifications.
Validation is the process of checking whether the software meets the user's needs and expectations.
A VnV plan helps to ensure the quality, reliability, and functionality of the software, as well as
to identify and correct any defects or errors before the software is released or deployed.


This document details the plan for verification and validation of \progname{}. Section
\ref{sec:general} provides an overview and objectives of this document. Section \ref{sec:plan}
discusses the methods of achieving the objectives. Section \ref{sec:system_test} covers the test
descriptions.

\wss{provide an introductory blurb and roadmap of the Verification and Validation plan}

\section{General Information} \label{sec:general}

\subsection{Summary}

\progname{} is an IMU-based attitude estimation algorithm. It consumes sensor data and produces and
estimate of the current orientation of the sensor relative to the Earth.

\wss{Say what software is being tested.  Give its name and a brief overview of its general
  functions.}

\subsection{Objectives}

The objectives of this Verification and Validation plan are the following:

\begin{itemize}
    \item Build confidence in the software correctness.
    \item Demonstrate the software's ability to accurately estimate orientation.
\end{itemize}

\wss{State what is intended to be accomplished.  The objective will be around the qualities that are
  most important for your project.  You might have something like: ``build confidence in the
  software correctness,'' ``demonstrate adequate usability.'' etc.  You won't list all the
  qualities, just those that are most important.}

\noindent
Objectives that are not included in the scope of this document:

\begin{itemize}
    \item Demonstration of adequate usability.
    \item Verification of external libraries.
\end{itemize}

It will be assumed that the documentation of \progname{} is adequate to facilitate adequate
usability for developers wishing to use it. Furthermore, it is assumed that external libraries have
been independently verified and validated.


\wss{You should also list the objectives that are out of scope.  You don't have the resources to do
everything, so what will you be leaving out.  For instance, if you are not going to verify the
quality of usability, state this.  It is also worthwhile to justify why the objectives are left
out.}

\wss{The objectives are important because they highlight that you are aware of limitations in your
resources for verification and validation.  You can't do everything, so what are you going to
prioritize?  As an example, if your system depends on an external library, you can explicitly state
that you will assume that external library has already been verified by its implementation team.}

\subsection{Relevant Documentation}

\wss{Reference relevant documentation.  This will definitely include your SRS and your other project
  documents (design documents, like MG, MIS, etc).  You can include these even before they are
  written, since by the time the project is done, they will be written.}

See the Software Requirements Specification \citep{SRS} for \progname{}, it details the goals,
requirements, assumptions, and theory of the software.

\wss{Don't just list the other documents.  You should explain why they are relevant and how they
relate to your VnV efforts.}

\section{Plan} \label{sec:plan}

This section will detail the plan for the verification of the documentation and software for
\progname{}. The primary items that will be verified are: SRS, design, VnV, implementation.

\wss{Introduce this section.   You can provide a roadmap of the sections to come.}

% \subsection{Verification and Validation Team}

% \wss{Your teammates.  Maybe your supervisor. You should do more than list names.  You should say
%   what each person's role is for the project's verification.  A table is a good way to summarize
%   this information.}

\subsection{SRS Verification Plan}

The SRS \citep{SRS} will be verified via feedback from a domain expert and secondary reviewers. The
following is a checklist for SRS review:

\begin{todolist}
    \item Problem Statement: confirm it is a valid statement.
    \item Goal Statement: confirm it is a valid goal.
    \item Physical system: check for missing descriptions.
    \item Assumptions: confirm assumptions are specific and meaningful.
    \item Theory: confirm instance models and all required theory is logical and complete enough for
    development.
    \item Requirements: confirm that the requirements are reasonably complete for testers and
    clients.
\end{todolist}


\wss{List any approaches you intend to use for SRS verification.  This may include ad hoc feedback
  from reviewers, like your classmates, or you may plan for something more rigorous/systematic.}

\wss{Maybe create an SRS checklist?}

\subsection{Design Verification Plan}

To verify the design, a domain expert and secondary reviewer will use the following checklist:

\begin{todolist}
    \item Inspect the design document for the following items:
    \begin{itemize}
        \item 
    \end{itemize}
    \item Create a high-level diagram that represents the relationships between modules. This is
    intended to find gaps in the design.
\end{todolist}

\wss{Plans for design verification}
\wss{The review will include reviews by your classmates}
\wss{Create a checklists?}

\subsection{Verification and Validation Plan Verification Plan}

This VnV plan will be verified via inspection from a domain expert and a secondary review. The
inspection will look for the following items:
\begin{todolist}
    \item Confirm all sections contain a verification checklist or description.
    \item Inspect system-level test cases for coverage of SRS requirements and goal statements.
\end{todolist}

\wss{The verification and validation plan is an artifact that should also be verified.  Techniques
for this include review and mutation testing.}
\wss{The review will include reviews by your classmates}
\wss{Create a checklists?}

\subsection{Implementation Verification Plan}

A combination of static and dynamic tests will be used to verify \progname{}. Cppcheck \citep{cppcheck} is a static code checking tool that will be employed to analyze the code for undefined behavior and poor design constructs.

System and unit level tests will be used to dynamically verify \progname{}. Section \ref{sec:system_test} outlines the system-level tests. Section \ref{sec:unit_test} details the unit tests.

\wss{You should at least point to the tests listed in this document and the unit testing plan.}
\wss{In this section you would also give any details of any plans for static verification of the
  implementation.  Potential techniques include code walkthroughs, code inspection, static
  analyzers, etc.}

\subsection{Automated Testing and Verification Tools}

\begin{itemize}
    \item Cmake \citep{cmake} is a build system tool that will be used to simplify building and testing \progname{}.
    \item GoogleTest is a unit testing framework that support c++ code. It also includes a mocking framework. See \url{https://github.com/google/googletest}.
    \item GCOV, LCOV are tools for calculating and displaying unit test coverage metrics.
    \item Valgring is a suite of tools for memory and performance profiling. It will be used to check for memory leaks and efficient memory utilization.
    \item Uncrustify is a tool that applies code formatting rules based on a configuration file. It will be used before every commit to the repo.
    \item Github PR checklists will help reviewers/developers see and check that all rules and tests are completed before code enter a protected branch (i.e. main).
    \item Github CI workflow will automate regression tests and checks that \progname{} builds are passing before a PR is merged into a protected branch.
    \item Docker is a mechanism to containerize applications. It will be used to ensure installability and usability of the project.
\end{itemize}

\wss{What tools are you using for automated testing.  Likely a unit testing framework and maybe a
  profiling tool, like ValGrind.  Other possible tools include a static analyzer, make, continuous
  integration tools, test coverage tools, etc.  Explain your plans for summarizing code coverage
  metrics. Linters are another important class of tools.  For the programming language you select,
  you should look at the available linters.  There may also be tools that verify that coding
  standards have been respected, like flake9 for Python.}

\wss{If you have already done this in the development plan, you can point to that document.}

\wss{The details of this section will likely evolve as you get closer to the implementation.}

\subsection{Software Validation Plan}

For the scope of this project, software validation will consist of benchmarking against existing
algorithms. The Root Mean Squared Error (RMSE) of \progname{} should lie within $\epsilon$ of the
RMSE of attitude estimators implemented by \citet{broad_code}. The data will come from the BROAD
dataset \citep{broad}.


\wss{If there is any external data that can be used for validation, you should point to it here.  If
  there are no plans for validation, you should state that here.} \wss{You might want to use review
  sessions with the stakeholder to check that the requirements document captures the right
  requirements.  Maybe task based inspection?} \wss{For teams without an external supervisor, user
  testing can serve the same purpose as a Rev 0 demo for the supervisor.}
\wss{This section might reference back to the SRS verification section.}

\section{System Test Description} \label{sec:system_test}
	
\subsection{Tests for Functional Requirements}

\wss{Subsets of the tests may be in related, so this section is divided into different areas.  If
  there are no identifiable subsets for the tests, this level of document structure can be removed.}
  \wss{Include a blurb here to explain why the subsections below cover the requirements.  References
  to the SRS would be good here.}

\subsubsection{Inputs and Outputs}

This section covers requirements R1 and R4 of the SRS. This includes the input and output
requirements for \progname{}. For input constraints see Section 4.2.8 of the SRS.

\wss{It would be nice to have a blurb here to explain why the subsections below cover the
  requirements.  References to the SRS would be good here.  If a section covers tests for input
  constraints, you should reference the data constraints table in the SRS.}
		
% \paragraph{Title for Test}

\begin{enumerate}

\item[\refstepcounter{testnum} \Tthetestnum \label{t:sys_io}:] \textbf{Input/Output Test} \\

    \textbf{Control:} Automatic
                
    \textbf{Initial State:} Uninitialized
                
    \begin{table}[!h]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
    Step & Input & Output \\ \hline
    1 &
        \begin{tabular}[c]{@{}l@{}}Initialize with: $ \Delta t = 1.0,{}^E\mathbf{b} = [1.0, 1.0,
        1.0],$ \\ $\gamma = 0.6, \text{outputType} = \text{Quat} $ \end{tabular} &
        \begin{tabular}[c]{@{}l@{}}Get params method matches\\ the inputs.\end{tabular} \\ \hline
    2 &
        \begin{tabular}[c]{@{}l@{}}One set of {[}accel, gyro, mag{]}\\ measurements.\end{tabular} &
        \begin{tabular}[c]{@{}l@{}}Orientation output should match\\ output format from
        above\end{tabular} \\ \hline
    3 &
    \begin{tabular}[c]{@{}l@{}}Initialize with: $ \Delta t = 1.0,{}^E\mathbf{b} = [1.0, 1.0, 1.0],$
    \\ $\gamma = 0.6, \text{outputType} = \text{Rot} $ \end{tabular} &
    \begin{tabular}[c]{@{}l@{}}Get params method matches\\ the inputs.\end{tabular} \\ \hline
    4 &
        \begin{tabular}[c]{@{}l@{}}One set of {[}accel, gyro, mag{]}\\ measurements.\end{tabular} &
        \begin{tabular}[c]{@{}l@{}}Orientation output should match\\ output format from
        above\end{tabular} \\ \hline
    5 &
        \begin{tabular}[c]{@{}l@{}}Initialize with: $ \Delta t = 1.0,{}^E\mathbf{b} = [1.0, 1.0,
        1.0],$ \\ $\gamma = 0.6, \text{outputType} = \text{Euler} $ \end{tabular} &
        \begin{tabular}[c]{@{}l@{}}Get params method matches\\ the inputs.\end{tabular} \\ \hline
    6 &
        \begin{tabular}[c]{@{}l@{}}One set of {[}accel, gyro, mag{]}\\ measurements.\end{tabular} &
        \begin{tabular}[c]{@{}l@{}}Orientation output should match\\ output format from
        above\end{tabular} \\ \hline
    \end{tabular}
    \end{table}

    \textbf{Test Case Derivation:} N/A
                
    \textbf{How test will be performed:} At each step, apply the inputs and assert the output.

\end{enumerate}

\subsubsection{Calculations} \label{sec:calc}
This section covers requirements R2 and R3 of the SRS. This includes the input and output
requirements for \progname{}. For input constraints see Section 4.2.8 of the SRS.

\begin{enumerate}

\item[\refstepcounter{testnum} \Tthetestnum \label{t:sys_calc_mag}:] \textbf{Orientation with Magnetometer} \\
   
    \textbf{Control:} Automatic
            
    \textbf{Initial State:} Uninitialized
    
    \begin{table}[!h]
        \centering
        \begin{tabular}{|l|l|l|}
        \hline
        Step & Input                                                                                       & Output \\ \hline
        1    & \begin{tabular}[c]{@{}l@{}}One complete set of valid \\ initialization inputs.\end{tabular} & -      \\ \hline
        2 & {[}accel, gyro, mag{]} measurements. & \begin{tabular}[c]{@{}l@{}}Calculate RMSE of output\\ vs. ground truth\end{tabular}     \\ \hline
        3    & \begin{tabular}[c]{@{}l@{}}Repeat step 2 for all\\ measurements in dataset\end{tabular}     & -      \\ \hline
        4 & Calculate average RMSE               & \begin{tabular}[c]{@{}l@{}}Assert average error is within\\ the tolerance.\end{tabular} \\ \hline
        \end{tabular}
        \end{table}
  
    \textbf{Test Case Derivation:} Benchmark dataset with labelled ground truth orientation for each
    set of sensor measurements.
    
    \textbf{How test will be performed:} At each step, apply the inputs and assert the output.

\item[\refstepcounter{testnum} \Tthetestnum \label{t:sys_calc_no_mag}:] \textbf{Orientation without Magnetometer}\\

    \textbf{Control:} Automatic
            
    \textbf{Initial State:} Uninitialized
    
    \begin{table}[!h]
        \centering
        \begin{tabular}{|l|l|l|}
        \hline
        Step & Input                                                                                       & Output \\ \hline
        1    & \begin{tabular}[c]{@{}l@{}}One complete set of valid \\ initialization inputs.\end{tabular} & -      \\ \hline
        2    & {[}accel, gyro, mag{]} measurements.                                                        & -      \\ \hline
        3 & {[}accel, gyro{]} measurements. & \begin{tabular}[c]{@{}l@{}}Calculate RMSE of output\\ vs. ground truth\end{tabular}     \\ \hline
        4    & \begin{tabular}[c]{@{}l@{}}Repeat step 3 for all\\ measurements in dataset\end{tabular}     & -      \\ \hline
        5 & Calculate average RMSE          & \begin{tabular}[c]{@{}l@{}}Assert average error is within\\ the tolerance.\end{tabular} \\ \hline
        \end{tabular}
        \end{table}
  
    \textbf{Test Case Derivation:} Benchmark dataset with labelled ground truth orientation for each
    set of sensor measurements.
    
    \textbf{How test will be performed:} At each step, apply the inputs and assert the output.
    
    \textbf{Note:} Only Pitch and Roll components are evaluated.
  
\end{enumerate}


\subsection{Tests for Nonfunctional Requirements}

\wss{The nonfunctional requirements for accuracy will likely just reference the appropriate
  functional tests from above.  The test cases should mention reporting the relative error for these
  tests.  Not all projects will necessarily have nonfunctional requirements related to accuracy}

\wss{Tests related to usability could include conducting a usability test and survey.  The survey
  will be in the Appendix.}

\wss{Static tests, review, inspections, and walkthroughs, will not follow the format for the tests
given below.}

\subsubsection{Accuracy}
		
% \paragraph{Title for Test}

\begin{enumerate}

\item[\refstepcounter{testnum} \Tthetestnum \label{t:a1}:] \textbf{test-a1} \\

    Type: Automatic
            
    This test refers to Section \ref{sec:calc}.
\end{enumerate}

\subsubsection{Usability}

\begin{enumerate}

\item[\refstepcounter{testnum} \Tthetestnum \label{t:u1}:] \textbf{test-u1} \\

    Type: Manual
            
    Initial State: N/A
            
    Input/Condition: N/A
            
    Output/Result: Successful integration with existing code.
            
    How test will be performed: A manual inspection of the code, specifically the header file. Can a developer read the documention/header file and correctly interact with the functions/objects?This test passes if this is true.
\end{enumerate}

\subsubsection{Maintainability}

\begin{enumerate}

\item[\refstepcounter{testnum} \Tthetestnum \label{t:m1}:] \textbf{test-m1} \\

    Type: Manual
            
    Initial State: N/A
            
    Input/Condition: N/A
            
    Output/Result: Successful addition of a likely change within the threshold defined in NFR3 of the SRS. 
            
    How test will be performed: If a likely change needs to be added, measure the development time and compare it to the total development time. If it is less than the threshold specified in the SRS, this test passes.
\end{enumerate}

\subsubsection{Portability}

\begin{enumerate}

\item[\refstepcounter{testnum} \Tthetestnum \label{t:p1}:] \textbf{test-p1} \\

    Type: Manual
            
    Initial State: N/A
            
    Input/Condition: One set of initialization inputs, 1 set of measurements
            
    Output/Result: Result of compilation and execution.
            
    How test will be performed: Build the project, initialize and input 1 set of measurements. Test is successful if the build has no errors and if the code produces the same result on Windows 11, macOS, and Ubuntu.
\end{enumerate}

\subsection{Traceability Between Test Cases and Requirements}

\begin{table}[!h]
    \centering
    \caption{Relation of Test Cases and Requirements.}
    \label{tab:traceability}
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|}
    \hline
      & R1 & R2 & R3 & R4 & NFR1 & NFR2 & NFR3 & NFR4 \\ \hline
    \testref{t:sys_io} & X  &    &    & X  &      &      &      &      \\ \hline
    \testref{t:sys_calc_mag} &    & X  &    &    & X    &      &      &      \\ \hline
    \testref{t:sys_calc_no_mag} &    &    & X  &    & X    &      &      &      \\ \hline
    \testref{t:a1} &    &    &    &    & X    &      &      &      \\ \hline
    \testref{t:u1} &    &    &    &    &      & X    &      &      \\ \hline
    \testref{t:m1} &    &    &    &    &      &      & X    &      \\ \hline
    \testref{t:p1} &    &    &    &    &      &      &      & X    \\ \hline
    \end{tabular}
    \end{table}

\wss{Provide a table that shows which test cases are supporting which requirements.}

\section{Unit Test Description} \label{sec:unit_test}

\wss{This section should not be filled in until after the MIS (detailed design document) has been
  completed.}

\wss{Reference your MIS (detailed design document) and explain your overall philosophy for test case
selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially layout your testing strategy here.  That is, you can explain
how tests will be selected for each module.  For instance, your test building approach could be test
cases for each access program, including one test for normal behaviour and as many tests as needed
for edge cases.  Rather than create the details of the input and output here, you could point to the
unit testing code.  For this to work, you code needs to be well-documented, with meaningful names
for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are developed by someone
  else, then you would say here if you aren't planning on verifying them.  There may also be modules
  that are part of your software, but have a lower priority for verification than others.  If this
  is the case, explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If appropriate specific
  modules can be verified by a non-testing based technique.  That can also be documented in this
  section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module. References to the
  MIS would be good.  You will want tests from a black box perspective and from a white box
  perspective.  Explain to the reader how the tests were selected.}

\begin{enumerate}

\item[\refstepcounter{utestnum} \Tthetestnum:] \textbf{Inputs: Invalid time} \\
                
    \textbf{Initial State:} ...
                
    \textbf{Input:} ...
            
    \textbf{Output:} ...

    \textbf{Test Case Derivation:} ...
            
    \textbf{How test will be performed:} ...

\item[\refstepcounter{utestnum} \Tthetestnum:] \textbf{Inputs: Invalid mag ref} \\
                
    \textbf{Initial State:} ...
                
    \textbf{Input:} ...
            
    \textbf{Output:} ...

    \textbf{Test Case Derivation:} ...
            
    \textbf{How test will be performed:} ...

\item[\refstepcounter{utestnum} \Tthetestnum:] \textbf{Inputs: Invalid gamma} \\
                
    \textbf{Initial State:} ...
                
    \textbf{Input:} ...
            
    \textbf{Output:} ...

    \textbf{Test Case Derivation:} ...
            
    \textbf{How test will be performed:} ...

\item[\refstepcounter{utestnum} \Tthetestnum:] \textbf{Inputs: Invalid output type} \\
                
    \textbf{Initial State:} ...
                
    \textbf{Input:} ...
            
    \textbf{Output:} ...

    \textbf{Test Case Derivation:} ...
            
    \textbf{How test will be performed:} ...

\item[\refstepcounter{utestnum} \Tthetestnum:] \textbf{Inputs: Valid} \\
                
    \textbf{Initial State:} ...
                
    \textbf{Input:} ...
            
    \textbf{Output:} ...

    \textbf{Test Case Derivation:} ...
            
    \textbf{How test will be performed:} ...
        
\end{enumerate}

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for performance, those test cases
  can go here.  In some projects, planning for nonfunctional tests of units will not be that
  relevant.}

\wss{These tests may involve collecting performance data from previously mentioned functional
  tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References, ../../refs/bib}

\newpage

\section{Appendix}

% This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS. Their values are defined in this
section for easy maintenance.

% \subsection{Usability Survey Questions?}

% \wss{This is a section that would be appropriate for some projects.}

% \newpage{} \section*{Appendix --- Reflection}

% The information in this section will be used to evaluate the team members on the graduate
% attribute of Lifelong Learning.  Please answer the following questions:




\end{document}