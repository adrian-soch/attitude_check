\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{enumitem,amssymb}
\usepackage{amsmath, mathtools}
\usepackage{amsfonts}
\usepackage{bm}
\hypersetup{ colorlinks, citecolor=blue, filecolor=black, linkcolor=red, urlcolor=blue }
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}

\begin{document}

\title{Project Title: System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X} \toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
2024/02/13 & 1.0 & Initial draft\\
\bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software. However, this does not
mean listing every verification and validation technique that has ever been devised.  The VnV plan
should also be a \textbf{feasible} plan. Execution of the plan should be possible with the time and
team available. If the full plan cannot be completed during the time available, it can either be
modified to ``fake it'', or a better solution is to add a section describing what work has been
completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before the design stage.
This means that the sections related to unit testing cannot initially be completed.  The sections
will be filled in after the design stage is complete.  the final version of the VnV plan should have
all sections filled in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}

For symbols and units see the Section 1 of the SRS \citep{SRS}. Table \ref{tab:abb} defines the abbreviations and acronyms used in this document.

% \renewcommand{\arraystretch}{1.2}
% \begin{tabular}{l l} 
%   \toprule		
%   \textbf{symbol} & \textbf{description}\\
%   \midrule 
%   T & Test\\
%   \bottomrule
% \end{tabular}\\

\begin{table}[!h]
  \centering
  \caption{Table of Abbreviations and Acronyms}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
    \toprule
    \textbf{symbol} & \textbf{description}\\
    \midrule 
    % TM & Theoretical Model\\
    IMU & Inertial Measurement Unit\\
    SRS & Software Requirements Specification\\
    VnV & Verification and Validation \\
    % MEMS & Micro-electromechanical System \\
    % NED & North-East-Down \\
    % WMM & World Magnetic Model. \\
    \bottomrule
\end{tabular}\\
\label{tab:abb}
\end{table}

\wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS \citep{SRS} tables, if
  appropriate}

\wss{Remove this section if it isn't needed}

\newpage

\pagenumbering{arabic}

\section{Introduction}

A verification and validation (VnV) plan is a document that describes the objectives, scope, methods, and criteria for verifying and validating a software product. Verification is the process of checking whether the software meets the specified requirements and design specifications. Validation is the process of checking whether the software meets the user's needs and expectations. A VnV plan helps to ensure the quality, reliability, and functionality of the software, as well as to identify and correct any defects or errors before the software is released or deployed.


This document details the plan for verification and validation of \progname{}. Section \ref{sec:general} provides an overview and objectives of this document. Section \ref{sec:plan} discusses the methods of achieving the objectives. Section \ref{sec:system_test} covers the test descriptions.

\wss{provide an introductory blurb and roadmap of the Verification and Validation plan}

\section{General Information} \label{sec:general}

\subsection{Summary}

\progname{} is an IMU-based attitude estimation algorithm. It consumes sensor data and produces and
estimate of the current orientation of the sensor relative to the Earth.

\wss{Say what software is being tested.  Give its name and a brief overview of its general
  functions.}

\subsection{Objectives}

The objectives of this Verification and Validation plan are the following:

\begin{itemize}
    \item Build confidence in the software correctness.
    \item Demonstrate the software's ability to accurately estimate orientation.
\end{itemize}

\wss{State what is intended to be accomplished.  The objective will be around the qualities that are
  most important for your project.  You might have something like: ``build confidence in the
  software correctness,'' ``demonstrate adequate usability.'' etc.  You won't list all the
  qualities, just those that are most important.}

\noindent
Objectives that are not included in the scope of this document:

\begin{itemize}
    \item Demonstration of adequate usability.
    \item Verification of external libraries.
\end{itemize}

It will be assumed that the documentation of \progname{} is adequate to facilitate adequate
usability for developers wishing to use it. Furthermore, it is assumed that external libraries have
been independently verified and validated.


\wss{You should also list the objectives that are out of scope.  You don't have the resources to do
everything, so what will you be leaving out.  For instance, if you are not going to verify the
quality of usability, state this.  It is also worthwhile to justify why the objectives are left
out.}

\wss{The objectives are important because they highlight that you are aware of limitations in your
resources for verification and validation.  You can't do everything, so what are you going to
prioritize?  As an example, if your system depends on an external library, you can explicitly state
that you will assume that external library has already been verified by its implementation team.}

\subsection{Relevant Documentation}

\wss{Reference relevant documentation.  This will definitely include your SRS and your other project
  documents (design documents, like MG, MIS, etc).  You can include these even before they are
  written, since by the time the project is done, they will be written.}

See the Software Requirements Specification \citep{SRS} for \progname{}, it details the goals, requirements, assumptions, and theory of the software.

\wss{Don't just list the other documents.  You should explain why they are relevant and how they
relate to your VnV efforts.}

\section{Plan} \label{sec:plan}

This section will detail the plan for the verification of the documentation and software for \progname{}. The primary items that will be verified are: SRS, design, VnV, implementation.

\wss{Introduce this section.   You can provide a roadmap of the sections to come.}

% \subsection{Verification and Validation Team}

% \wss{Your teammates.  Maybe your supervisor. You should do more than list names.  You should say
%   what each person's role is for the project's verification.  A table is a good way to summarize
%   this information.}

\subsection{SRS Verification Plan}

The SRS \citep{SRS} will be verified via feedback from a domain expert and secondary reviewers. The following is a checklist for SRS review:

\begin{todolist}
    \item Problem Statement: confirm it is a valid statement.
    \item Goal Statement: confirm it is a valid goal.
    \item Physical system: check for missing descriptions.
    \item Assumptions: confirm assumptions are specific and meaningful.
    \item Theory: confirm instance models and all required theory is logical and complete enough for development.
    \item Requirements: confirm that the requirements are reasonably complete for testers and clients.
\end{todolist}


\wss{List any approaches you intend to use for SRS verification.  This may include ad hoc feedback
  from reviewers, like your classmates, or you may plan for something more rigorous/systematic.}

\wss{Maybe create an SRS checklist?}

\subsection{Design Verification Plan}

To verify the design, a domain expert and secondary reviewer will use the following checklist:

\begin{todolist}
    \item Inspect the design document for the following items:
    \begin{itemize}
        \item 
    \end{itemize}
    \item Create a high-level diagram that represents the relationships between modules. This is intended to find gaps in the design.
\end{todolist}

\wss{Plans for design verification}
\wss{The review will include reviews by your classmates}
\wss{Create a checklists?}

\subsection{Verification and Validation Plan Verification Plan}

This VnV plan will be verified via inspection from a domain expert and a secondary review. The inspection will look for the following items:
\begin{todolist}
    \item Confirm all sections contain a verification checklist or description.
    \item Inspect system-level test cases for coverage of SRS requirements and goal statements.
\end{todolist}

\wss{The verification and validation plan is an artifact that should also be verified.  Techniques
for this include review and mutation testing.}
\wss{The review will include reviews by your classmates}
\wss{Create a checklists?}

\subsection{Implementation Verification Plan}

\wss{You should at least point to the tests listed in this document and the unit testing plan.}
\wss{In this section you would also give any details of any plans for static verification of the
  implementation.  Potential techniques include code walkthroughs, code inspection, static
  analyzers, etc.}

\subsection{Automated Testing and Verification Tools}

\begin{itemize}
    \item GoogleTest \url{https://github.com/google/googletest}
    \item Gcov, lCov for calculating and displaying test coverage
    \item Valgring/memcheck for RAM usage and checking for memory leaks
    \item Cmake for building
    \item Uncrustify for automatic formatting
    \item CppCheck for static analysis
    \item Github PR checklists
    \item Github CI workflow for automated regression test and checking thats builds pass before entering the main branch
    \item Docker
\end{itemize}

\wss{What tools are you using for automated testing.  Likely a unit testing framework and maybe a
  profiling tool, like ValGrind.  Other possible tools include a static analyzer, make, continuous
  integration tools, test coverage tools, etc.  Explain your plans for summarizing code coverage
  metrics. Linters are another important class of tools.  For the programming language you select,
  you should look at the available linters.  There may also be tools that verify that coding
  standards have been respected, like flake9 for Python.}

\wss{If you have already done this in the development plan, you can point to that document.}

\wss{The details of this section will likely evolve as you get closer to the implementation.}

\subsection{Software Validation Plan}

For the scope of this project, software validation will consist of benchmarking against existing algorithms. The Root Mean Squared Error (RMSE) of \progname{} should lie within $\epsilon$ of the RMSE of attitude estimators implemented by \citet{broad_code}. The data will come from the BROAD dataset \citep{broad}.


\wss{If there is any external data that can be used for validation, you should point to it here.  If
  there are no plans for validation, you should state that here.}
\wss{You might want to use review sessions with the stakeholder to check that the requirements
document captures the right requirements.  Maybe task based inspection?}
\wss{For teams without an external supervisor, user testing can serve the same purpose as a Rev 0
demo for the supervisor.}
\wss{This section might reference back to the SRS verification section.}

\section{System Test Description} \label{sec:system_test}
	
\subsection{Tests for Functional Requirements}

\wss{Subsets of the tests may be in related, so this section is divided into different areas.  If
  there are no identifiable subsets for the tests, this level of document structure can be removed.}
\wss{Include a blurb here to explain why the subsections below cover the requirements.  References
  to the SRS would be good here.}

\subsubsection{Inputs and Outputs}

This section covers requirements R1 and R4 of the SRS. This includes the input and output requirements for \progname{}. For input constraints see Section 4.2.8 of the SRS.

\wss{It would be nice to have a blurb here to explain why the subsections below cover the
  requirements.  References to the SRS would be good here.  If a section covers tests for input
  constraints, you should reference the data constraints table in the SRS.}
		
% \paragraph{Title for Test}

\begin{enumerate}

\item{\textbf{Input/Output Test}\\}

    \textbf{Control:} Automatic
                
    \textbf{Initial State:} Uninitialized
                
    \textbf{Input:} One complete valid set of initialization inputs, one set of measurements from: [Accelerometer, Gyroscope, Magnetometer].
                
    \textbf{Output:} Attitude Estimator (AE) object is initialized. Get parameters method returns the same parameters that were given. Orientation output is in the expected format.

    \textbf{Test Case Derivation:} N/A
                
    \textbf{How test will be performed:} Test fixture will create and initialize the AE.

\item{Periodic Input Test\\}
              
    \textbf{Initial State:} Initialized
                
    \textbf{Input:}
                
    \textbf{Output:} \wss{The expected result for the given inputs}

    \textbf{Test Case Derivation:} \wss{Justify the expected value given in the Output field}

    \textbf{How test will be performed:}

\end{enumerate}

\subsubsection{Calculations}
\begin{enumerate}
  
\item{Periodic Input Test\\}
            
  \textbf{Initial State: }
              
  \textbf{Input:}
              
  \textbf{Output:} \wss{The expected result for the given inputs}
  
  \textbf{Test Case Derivation:} \wss{Justify the expected value given in the Output field}
  
  \textbf{How test will be performed:}
  
  \textbf{Note:} See Section 4.2.8 of the SRS for input constraints.
  
\end{enumerate}


\subsection{Tests for Nonfunctional Requirements}

\wss{The nonfunctional requirements for accuracy will likely just reference the appropriate
  functional tests from above.  The test cases should mention reporting the relative error for these
  tests.  Not all projects will necessarily have nonfunctional requirements related to accuracy}

\wss{Tests related to usability could include conducting a usability test and survey.  The survey
  will be in the Appendix.}

\wss{Static tests, review, inspections, and walkthroughs, will not follow the format for the tests
given below.}

\subsubsection{Accuracy}
		
% \paragraph{Title for Test}

\begin{enumerate}

\item{test-id1\\}

  Type: Functional, Dynamic, Manual, Static etc.
            
  Initial State: 
            
  Input/Condition: 
            
  Output/Result: 
            
  How test will be performed: 
\end{enumerate}

\subsubsection{Portability}

\begin{enumerate}

  \item{test-id1\\}
  
    Type: Functional, Dynamic, Manual, Static etc.
              
    Initial State: 
              
    Input/Condition: 
              
    Output/Result: 
              
    How test will be performed: 
  \end{enumerate}

\subsection{Traceability Between Test Cases and Requirements}

\wss{Provide a table that shows which test cases are supporting which requirements.}

\section{Unit Test Description}

\wss{This section should not be filled in until after the MIS (detailed design document) has been
  completed.}

\wss{Reference your MIS (detailed design document) and explain your overall philosophy for test case
selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially layout your testing strategy here.  That is, you can explain
how tests will be selected for each module.  For instance, your test building approach could be test
cases for each access program, including one test for normal behaviour and as many tests as needed
for edge cases.  Rather than create the details of the input and output here, you could point to the
unit testing code.  For this to work, you code needs to be well-documented, with meaningful names
for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are developed by someone
  else, then you would say here if you aren't planning on verifying them.  There may also be modules
  that are part of your software, but have a lower priority for verification than others.  If this
  is the case, explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If appropriate specific
  modules can be verified by a non-testing based technique.  That can also be documented in this
  section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module. References to the
  MIS would be good.  You will want tests from a black box perspective and from a white box
  perspective.  Explain to the reader how the tests were selected.}

\begin{enumerate}


    \item{\textbf{Input Test}\\}
                
    \textbf{Initial State:} Uninitialized
                
      \textbf{Input:} \begin{align*}
        \Delta t &= 1.0\\
        {}^E\mathbf{b} &= [1.0, 1.0, 1.0]\\
        \gamma &= 0.6\\
        \text{outputTypeEnum} &= \text{Quaternion}
      \end{align*}
    
      One set of measurements from: [Accelerometer, Gyroscope, Magnetometer]
                
      \textbf{Output:} Attitude Estimator (AE) object is initialized. Get parameters method returns the same parameters that were given.
    
      \textbf{Test Case Derivation:} N/A
                
      \textbf{How test will be performed:} Test fixture will create and initialize the AE.
    
      \textbf{Note:} See Section 4.2.8 of the SRS for input constraints.
    
    \item{Periodic Input Test\\}
                
    \textbf{Initial State:} Initialized
                
    \textbf{Input:}
                
    \textbf{Output:} \wss{The expected result for the given inputs}
    
    \textbf{Test Case Derivation:} \wss{Justify the expected value given in the Output field}
    
    \textbf{How test will be performed:}
    
    \textbf{Note:} See Section 4.2.8 of the SRS for input constraints.
    

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\item{...\\}
    
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for performance, those test cases
  can go here.  In some projects, planning for nonfunctional tests of units will not be that
  relevant.}

\wss{These tests may involve collecting performance data from previously mentioned functional
  tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References, ../../refs/bib}

\newpage

\section{Appendix}

% This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS. Their values are defined in this
section for easy maintenance.

% \subsection{Usability Survey Questions?}

% \wss{This is a section that would be appropriate for some projects.}

% \newpage{}
% \section*{Appendix --- Reflection}

% The information in this section will be used to evaluate the team members on the graduate attribute
% of Lifelong Learning.  Please answer the following questions:




\end{document}