\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{enumitem,amssymb}
\usepackage{amsmath, mathtools}
\usepackage{amsfonts}
\usepackage{bm}
\usepackage{float}
\usepackage{pdflscape}
\hypersetup{ colorlinks, citecolor=blue, filecolor=cyan, linkcolor=red, urlcolor=blue }
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}

\newcommand{\testref}[1]{T\ref{#1}} \newcounter{testnum}
\newcommand{\Tthetestnum}{T\thetestnum}

\newcommand{\utestref}[1]{UT\ref{#1}} \newcounter{utestnum}
\newcommand{\UTthetestnum}{UT\theutestnum}

\begin{document}

\title{Project Title: System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X} \toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
2024/02/13 & 1.0 & Initial draft\\
\bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software. However, this does not
mean listing every verification and validation technique that has ever been devised.  The VnV plan
should also be a \textbf{feasible} plan. Execution of the plan should be possible with the time and
team available. If the full plan cannot be completed during the time available, it can either be
modified to ``fake it'', or a better solution is to add a section describing what work has been
completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before the design stage.
This means that the sections related to unit testing cannot initially be completed.  The sections
will be filled in after the design stage is complete.  the final version of the VnV plan should have
all sections filled in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}

For symbols and units see the Section 1 of the SRS \citep{SRS}. Table \ref{tab:abb} defines the
abbreviations and acronyms used in this document.

% \renewcommand{\arraystretch}{1.2} \begin{tabular}{l l} \toprule        
%   \textbf{symbol} & \textbf{description}\\
%   \midrule T & Test\\
%   \bottomrule \end{tabular}\\

\begin{table}[!h]
  \centering
  \caption{Table of Abbreviations and Acronyms}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
    \toprule
    \textbf{symbol} & \textbf{description}\\
    \midrule 
    % TM & Theoretical Model\\
    accel & Accelerometer \\
    gyro & Gyroscope \\
    IMU & Inertial Measurement Unit\\
    mag & Magnetometer \\
    SRS & Software Requirements Specification\\
    VnV & Verification and Validation \\
    % MEMS & Micro-electromechanical System \\
    % NED & North-East-Down \\
    % WMM & World Magnetic Model. \\
    \bottomrule
\end{tabular}\\
\label{tab:abb}
\end{table}

\wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS \citep{SRS} tables, if
  appropriate}

\wss{Remove this section if it isn't needed}

\newpage

\pagenumbering{arabic}

\section{Introduction}

A Verification and Validation (VnV) plan is a document that describes the objectives, scope,
methods, and criteria for verifying and validating a software product. Verification is the process
of checking whether the software meets the specified requirements and design specifications.
Validation is the process of checking whether the software meets the user's needs and expectations.
A VnV plan helps to ensure the quality, reliability, and functionality of the software, as well as
to identify and correct any defects or errors before the software is released or deployed.


This document details the plan for verification and validation of \progname{}. Section
\ref{sec:general} provides an overview and objectives of this document. Section \ref{sec:plan}
discusses the methods of achieving the objectives. Section \ref{sec:system_test} covers the test
descriptions.

\wss{provide an introductory blurb and roadmap of the Verification and Validation plan}

\section{General Information} \label{sec:general}

\subsection{Summary}

\progname{} is an IMU-based attitude estimation algorithm. It consumes sensor data and produces and
estimate of the current orientation of the sensor relative to the Earth.

\wss{Say what software is being tested.  Give its name and a brief overview of its general
  functions.}

\subsection{Objectives}

The objectives of this Verification and Validation plan are the following:

\begin{itemize}
    \item Build confidence in the software correctness.
    \item Demonstrate the software's ability to accurately estimate orientation.
\end{itemize}

\wss{State what is intended to be accomplished.  The objective will be around the qualities that are
  most important for your project.  You might have something like: ``build confidence in the
  software correctness,'' ``demonstrate adequate understandability.'' etc.  You won't list all the
  qualities, just those that are most important.}

\noindent
Objectives that are not included in the scope of this document:

\begin{itemize}
    \item Demonstration of adequate understandability.
    \item Verification of external libraries.
\end{itemize}

It will be assumed that the documentation of \progname{} is adequate to facilitate adequate
understandability for developers wishing to use it. Furthermore, it is assumed that external
libraries have been independently verified and validated.


\wss{You should also list the objectives that are out of scope.  You don't have the resources to do
everything, so what will you be leaving out.  For instance, if you are not going to verify the
quality of understandability, state this.  It is also worthwhile to justify why the objectives are
left out.}

\wss{The objectives are important because they highlight that you are aware of limitations in your
resources for verification and validation.  You can't do everything, so what are you going to
prioritize?  As an example, if your system depends on an external library, you can explicitly state
that you will assume that external library has already been verified by its implementation team.}

\subsection{Relevant Documentation}

\wss{Reference relevant documentation.  This will definitely include your SRS and your other project
  documents (design documents, like MG, MIS, etc).  You can include these even before they are
  written, since by the time the project is done, they will be written.}

See the Software Requirements Specification \citep{SRS} for \progname{}, it details the goals,
requirements, assumptions, and theory of the software.

Further reading about attitude estimation can be found at \citep{madgwick_ecient_nodate}. Information regarding evaluation of attitude estimation algorithms can be found at \citep{broad}.

\wss{Don't just list the other documents.  You should explain why they are relevant and how they
relate to your VnV efforts.}

\section{Plan} \label{sec:plan}

This section will detail the plan for the verification of the documentation and software for
\progname{}. The primary items that will be verified are: SRS, design, VnV, implementation.

\wss{Introduce this section.   You can provide a roadmap of the sections to come.}

% \subsection{Verification and Validation Team}

% \wss{Your teammates.  Maybe your supervisor. You should do more than list names.  You should say
%   what each person's role is for the project's verification.  A table is a good way to summarize
%   this information.}

\subsection{SRS Verification Plan}

The SRS \citep{SRS} will be verified via feedback from a domain expert and assigned secondary
reviewer. The following is a checklist for SRS review derived from \citep{wiegers2002peer}:

\begin{todolist}
    \item Are all internal cross-references to other requirements correct?
    \item Are all requirements written at a consistent and appropriate level of detail?
    \item Do any requirements conflict with or duplicate other requirements?
    \item Is each requirement written in clear, concise, unambiguous language?
    \item Is each requirement verifiable by testing, demonstration, review, or analysis?
    \item Are all requirements actually requirements, not design or implementation solutions?
\end{todolist}


\wss{List any approaches you intend to use for SRS verification.  This may include ad hoc feedback
  from reviewers, like your classmates, or you may plan for something more rigorous/systematic.}

\wss{Maybe create an SRS checklist?}

\subsection{Design Verification Plan}

To verify the design, feedback will be collected from domain expert and an assigned secondary
reviewer will use the following checklist:

\begin{todolist}
    \item Create a high-level diagram that represents the relationships between modules based on the
    Module Guide (MG) and Module Interface Specification (MIS) [to be cited]. This is intended to
    find gaps in the design.
\end{todolist}

\wss{Plans for design verification}
\wss{The review will include reviews by your classmates}
\wss{Create a checklists?}

\subsection{Verification and Validation Plan Verification Plan}

This VnV plan will be verified via inspection from a domain expert and a secondary review. The
inspection will look for the following items:
\begin{todolist}
    \item Confirm all sections contain a verification checklist or description.
    \item Inspect system-level test cases for coverage of SRS requirements and goal statements.
    \item Inspect all test cases for complete definitions.
\end{todolist}

\wss{The verification and validation plan is an artifact that should also be verified.  Techniques
for this include review and mutation testing.}
\wss{The review will include reviews by your classmates}
\wss{Create a checklists?}

\subsection{Implementation Verification Plan}

A combination of static and dynamic tests will be used to verify \progname{}. Cppcheck
\citep{cppcheck} is a static code checking tool that will be employed to analyze the code for
undefined behaviour and poor design constructs.

System and unit level tests will be used to dynamically verify \progname{}. Section
\ref{sec:system_test} outlines the system-level tests. Section \ref{sec:unit_test} details the unit
tests.

\wss{You should at least point to the tests listed in this document and the unit testing plan.}
\wss{In this section you would also give any details of any plans for static verification of the
  implementation.  Potential techniques include code walkthroughs, code inspection, static
  analyzers, etc.}

\subsection{Automated Testing and Verification Tools}

\begin{itemize}
    \item Cmake \citep{cmake} is a build system tool that will be used to simplify building and
    testing.
    \item GoogleTest is a unit testing framework that support c++ code. It also includes a mocking
    framework. See \url{https://github.com/google/googletest}.
    \item GCOV, LCOV are tools for calculating and displaying unit test coverage metrics.
    \item Valgrind is a suite of tools for memory and performance profiling. It will be used to
    check for memory leaks and efficient memory utilization.
    \item Uncrustify is a tool that applies code formatting rules based on a configuration file. It
    will be used before every commit to the repo.
    \item GitHub PR checklists will help reviewers/developers see and check that all rules and tests
    are completed before code is merged into a protected branch (i.e. main).
    \item GitHub CI workflow will automate regression tests and checks that \progname{} builds are
    passing before a code is merged into a protected branch.
    \item Docker is a mechanism to containerize applications. It will be used to ensure
    installability and understandability of the project.
\end{itemize}

\wss{What tools are you using for automated testing.  Likely a unit testing framework and maybe a
  profiling tool, like ValGrind.  Other possible tools include a static analyzer, make, continuous
  integration tools, test coverage tools, etc.  Explain your plans for summarizing code coverage
  metrics. Linters are another important class of tools.  For the programming language you select,
  you should look at the available linters.  There may also be tools that verify that coding
  standards have been respected, like flake9 for Python.}

\wss{If you have already done this in the development plan, you can point to that document.}

\wss{The details of this section will likely evolve as you get closer to the implementation.}

\subsection{Software Validation Plan}

For the scope of this project, software validation will consist of benchmarking against existing
algorithms. The Root Mean Squared Error (RMSE) of \progname{} should lie within $\epsilon$ of the
RMSE of attitude estimators implemented by \citet{broad_code}. The data will come from the BROAD
dataset \citep{broad}.

The derivation of RMSE can be found in the Section \ref{sec:cal}.

\wss{If there is any external data that can be used for validation, you should point to it here.  If
  there are no plans for validation, you should state that here.} \wss{You might want to use review
  sessions with the stakeholder to check that the requirements document captures the right
  requirements.  Maybe task based inspection?} \wss{For teams without an external supervisor, user
  testing can serve the same purpose as a Rev 0 demo for the supervisor.}
\wss{This section might reference back to the SRS verification section.}

\section{System Test Description} \label{sec:system_test}
	
\subsection{Tests for Functional Requirements}

\wss{Subsets of the tests may be in related, so this section is divided into different areas.  If
  there are no identifiable subsets for the tests, this level of document structure can be removed.}
  \wss{Include a blurb here to explain why the subsections below cover the requirements.  References
  to the SRS would be good here.}

\subsubsection{Inputs and Outputs}

This section covers requirements R1 and R4 of the SRS. This includes the input and output
requirements for \progname{}. For input constraints see Section 4.2.8 of the SRS.

\wss{It would be nice to have a blurb here to explain why the subsections below cover the
  requirements.  References to the SRS would be good here.  If a section covers tests for input
  constraints, you should reference the data constraints table in the SRS.}
		
% \paragraph{Title for Test}

\begin{enumerate}

\item[\refstepcounter{testnum} \Tthetestnum \label{t:sys_io}:] \textbf{Input/Output Test} \\


    \textbf{Control:} Automatic
                
    \textbf{Initial State:} Uninitialized
                
    \begin{table}[H]
    \centering
    \begin{tabular}{|l|l|l|}
    \hline
    Step & Input & Output \\ \hline
    1 &
        \begin{tabular}[c]{@{}l@{}}Initialize with: $ \Delta t = 1.0,{}^E\mathbf{b} = [1.0, 1.0,
        1.0],$ \\ $\gamma = 0.6, \text{outputType} = \text{Quat} $ \end{tabular} &
        \begin{tabular}[c]{@{}l@{}l@{}}Assert ``get params" method equals\\ $ \Delta t =
        1.0,{}^E\mathbf{b} = [1.0, 1.0, 1.0],$ \\ $\gamma = 0.6, \text{outputType} = \text{Quat}
        $\end{tabular} \\ \hline
    2 &
        \begin{tabular}[c]{@{}l@{}l@{}}Call ``update" with: \\${}^S\mathbf{a}_t = [0,0,9.81], \quad$
          ${}^S\mathbf{\bm{\omega}}_t = [0,0,0]$ \\ ${}^S\mathbf{m}_t = [16676.8, -3050.9, 49916.9]$
          \end{tabular} &
        \begin{tabular}[c]{@{}l@{}}Assert ``update" output is a \\ normalized quaternion $\in
        \mathbb{R}^4$. \end{tabular} \\ \hline
    3 &
    \begin{tabular}[c]{@{}l@{}}Initialize with: $ \Delta t = 1.0,{}^E\mathbf{b} = [1.0, 1.0, 1.0],$
      \\ $\gamma = 0.6, \text{outputType} = \text{Rot} $ \end{tabular} &
      \begin{tabular}[c]{@{}l@{}l@{}}Assert ``get params" method equals\\ $ \Delta t =
      1.0,{}^E\mathbf{b} = [1.0, 1.0, 1.0],$ \\ $\gamma = 0.6, \text{outputType} = \text{Rot}
      $\end{tabular} \\ \hline
    4 &
    \begin{tabular}[c]{@{}l@{}l@{}}Call ``update" with: \\${}^S\mathbf{a}_t = [0,0,9.81], \quad$
      ${}^S\mathbf{\bm{\omega}}_t = [0,0,0]$ \\ ${}^S\mathbf{m}_t = [16676.8, -3050.9, 49916.9]$
      \end{tabular} &
    \begin{tabular}[c]{@{}l@{}}Assert ``update" output is a \\ matrix $\in \mathbb{R}^{3 \times 3}$.
    \end{tabular} \\ \hline
    5 &
    \begin{tabular}[c]{@{}l@{}}Initialize with: $ \Delta t = 1.0,{}^E\mathbf{b} = [1.0, 1.0, 1.0],$
      \\ $\gamma = 0.6, \text{outputType} = \text{Euler} $ \end{tabular} &
      \begin{tabular}[c]{@{}l@{}l@{}}Assert ``get params" method equals\\ $ \Delta t =
      1.0,{}^E\mathbf{b} = [1.0, 1.0, 1.0],$ \\ $\gamma = 0.6, \text{outputType} = \text{Euler}
      $\end{tabular} \\ \hline
    6 &
    \begin{tabular}[c]{@{}l@{}l@{}}Call ``update" with: \\${}^S\mathbf{a}_t = [0,0,9.81], \quad$
      ${}^S\mathbf{\bm{\omega}}_t = [0,0,0]$ \\ ${}^S\mathbf{m}_t = [16676.8, -3050.9, 49916.9]$
      \end{tabular} &
    \begin{tabular}[c]{@{}l@{}}Assert ``update" output \\ is vector $\in \mathbb{R}^3$.
    \end{tabular} \\ \hline
    \end{tabular}
    \end{table}

    \textbf{Test Case Derivation:} N/A
                
    \textbf{How test will be performed:} At each step, apply the inputs and assert the output.

\end{enumerate}

\subsubsection{Calculations} \label{sec:calc} This section covers requirements R2 and R3 of the SRS.
This includes the input and output requirements for \progname{}. For input constraints see Section
4.2.8 of the SRS.

The accuracy metric is described in \cite{broad}, it defines the difference between two quaternions
(\ref{eqn:diff_quat}), the total error (\ref{eqn:error}), and the RMSE (\ref{eqn:rmse}):
\begin{align}
    \mathbf{q}_{e, t} =& {}^S_E\mathbf{q}^{*}_{gt, t}  {}^S_E\mathbf{q}_{est, t} = [q_w, q_x, q_y, q_z]^T \label{eqn:diff_quat} \\
    e_q =& 2 \arccos(|q_w|)  \label{eqn:error}\\
    \text{RMSE} =& \sqrt{\cfrac{\sum_{t=0}^n(e)^2}{n}}  \label{eqn:rmse}
\end{align}

Often computing separate errors for the heading and inclination components provides better incite into the performance. When magnetometer data is not available, the header error will be much larger than the other sources of error.

First the difference between the estimate and the ground truth mus tbe expressed in the Earth frame (\ref{eqn:diff_quat2}). The equation for the heading only error is defined in (\ref{eqn:heading_err}) and the inclination only error is defined by (\ref{eqn:inc_error}):

\begin{align}
    {}^E \mathbf{q}_{e,t} =& {}^S_E\mathbf{q}_{est, t} {}^S_E\mathbf{q}^{*}_{gt, t} = [q_w, q_x, q_y, q_z]^T \label{eqn:diff_quat2} \\ 
    e_{h} =& 2 \arctan(|\cfrac{q_z}{q_w}|) \label{eqn:heading_err} \\
    e_{i} =&  2 \arccos(\sqrt{q_w^2 + q_z^2}) \label{eqn:inc_error}
\end{align}

\textbf{Note:}Ground truth quaternions and their corresponding sensor measurements are provided in the Appendix \ref{sec:imu_data} for quick reference, however the tester should use the full dataset found in \citep{broad_code}.

\begin{enumerate}

\item[\refstepcounter{testnum} \Tthetestnum \label{t:sys_calc_mag}:] \textbf{Orientation with
Magnetometer} \\
   
    \textbf{Control:} Automatic
            
    \textbf{Initial State:} Uninitialized
    
    \begin{table}[H]
        \centering
        \begin{tabular}{|l|l|l|}
        \hline
        Step & Input & Output \\ \hline
        1    & \begin{tabular}[c]{@{}l@{}l@{}}Initialize with: $ \Delta t = 10.0, \gamma = 0.6,$ \\$
        {}^E\mathbf{b} = [16676.8, -3050.9, 49916.9],$\\ $\text{outputType} = \text{Quat} $
        \end{tabular} & - \\ \hline
        2 & \begin{tabular}[c]{@{}l@{}}Call ``update" and loop through \\ measurements in Table
        \ref{tab:gt_data} \end{tabular} & Assert output quat is normalized     \\ \hline
        3 & \begin{tabular}[c]{@{}l@{}}Calculate RMSE for $e_q, e_h, e_i$\\ vs. ground truth in table
            \ref{tab:gt_label}\end{tabular}             & \begin{tabular}[c]{@{}l@{}}Assert average
            error is within\\ the tolerance.\end{tabular} \\ \hline
        \end{tabular}
        \end{table}
  
    \textbf{Test Case Derivation:} Benchmark dataset with labelled ground truth orientation for each
    set of sensor measurements.
    
    \textbf{How test will be performed:} At each step, apply the inputs and assert the output.

\item[\refstepcounter{testnum} \Tthetestnum \label{t:sys_calc_no_mag}:] \textbf{Orientation without
Magnetometer}\\

    \textbf{Control:} Automatic
            
    \textbf{Initial State:} Uninitialized
    
    \begin{table}[H]
        \centering
        \begin{tabular}{|l|l|l|}
        \hline
        Step & Input & Output \\ \hline
        1    & \begin{tabular}[c]{@{}l@{}l@{}}Initialize with: $ \Delta t = 10.0, \gamma = 0.6,$ \\$
        {}^E\mathbf{b} = [16676.8, -3050.9, 49916.9],$\\ $\text{outputType} = \text{Quat} $
        \end{tabular} & - \\ \hline
        2 & \begin{tabular}[c]{@{}l@{}}Call ``update" with \\ measurements at $t=0$ from Table
        \ref{tab:gt_data} \end{tabular} & Assert output quat is normalized     \\ \hline
        3 & \begin{tabular}[c]{@{}l@{}}Call ``update" and loop through \\ \{accel, gyro\} measurements
        in Table \ref{tab:gt_data} \end{tabular} & Assert output quat is normalized     \\ \hline
        4 & \begin{tabular}[c]{@{}l@{}}Calculate RMSE of $e_q, e_h, e_i$\\ vs. ground truth in table
            \ref{tab:gt_label}\end{tabular}             & \begin{tabular}[c]{@{}l@{}}Assert average
            error is within\\ the tolerance.\end{tabular} \\ \hline
        \end{tabular}
        \end{table}
  
    \textbf{Test Case Derivation:} Benchmark dataset with labelled ground truth orientation for each
    set of sensor measurements.
    
    \textbf{How test will be performed:} At each step, apply the inputs and assert the output.
    
    \textbf{Note:} Only Pitch and Roll components are evaluated.
  
\end{enumerate}


\subsection{Tests for Nonfunctional Requirements}

\wss{The nonfunctional requirements for accuracy will likely just reference the appropriate
  functional tests from above.  The test cases should mention reporting the relative error for these
  tests.  Not all projects will necessarily have nonfunctional requirements related to accuracy}

\wss{Tests related to understandability could include conducting a understandability test and
  survey.  The survey will be in the Appendix.}

\wss{Static tests, review, inspections, and walkthroughs, will not follow the format for the tests
given below.}

\subsubsection{Accuracy}
		
% \paragraph{Title for Test}

\begin{enumerate}

\item[\refstepcounter{testnum} \Tthetestnum \label{t:a1}:] \textbf{test-a1} \\

    Type: Automatic
            
    This test refers to Section \ref{sec:calc}.
\end{enumerate}

\subsubsection{Understandability}

\begin{enumerate}

\item[\refstepcounter{testnum} \Tthetestnum \label{t:u1}:] \textbf{test-u1} \\

    Type: Manual
            
    Initial State: N/A
            
    Input/Condition: N/A
            
    Output/Result: Successful integration of \progname{} into a project. This represents how typical
    users interact with the software.
            
    How test will be performed: A manual inspection of the code, specifically the header file. Is
    each input documented/commented? This test passes if this is true.
\end{enumerate}

\subsubsection{Maintainability}

\begin{enumerate}

\item[\refstepcounter{testnum} \Tthetestnum \label{t:m1}:] \textbf{test-m1} \\

    Type: Manual
            
    Initial State: N/A
            
    Input/Condition: N/A
            
    Output/Result: Successful addition of a likely change within the threshold defined in NFR3 of
    the SRS. 
            
    How test will be performed: If a likely change needs to be added, measure the development time
    and compare it to the total development time. If it is less than the threshold specified in the
    SRS, this test passes.
\end{enumerate}

\subsubsection{Portability}

\begin{enumerate}

\item[\refstepcounter{testnum} \Tthetestnum \label{t:p1}:] \textbf{test-p1} \\

    Type: Manual
            
    Initial State: N/A
            
    Input/Condition: One set of initialization inputs, 1 set of measurements
            
    Output/Result: Result of compilation and execution.
            
    How test will be performed: Build the project, initialize and input 1 set of measurements. Test
    is successful if the build has no errors and if the code produces the same result on Windows 11,
    macOS, and Ubuntu.
\end{enumerate}

\subsection{Traceability Between Test Cases and Requirements}

\begin{table}[!h]
    \centering
    \caption{Relation of Test Cases and Requirements.}
    \label{tab:traceability}
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|}
    \hline
      & R1 & R2 & R3 & R4 & NFR1 & NFR2 & NFR3 & NFR4 \\ \hline
    \testref{t:sys_io} & X  &    &    & X  &      &      &      &      \\ \hline
    \testref{t:sys_calc_mag} &    & X  &    &    & X    &      &      &      \\ \hline
    \testref{t:sys_calc_no_mag} &    &    & X  &    & X    &      &      &      \\ \hline
    \testref{t:a1} &    &    &    &    & X    &      &      &      \\ \hline
    \testref{t:u1} &    &    &    &    &      & X    &      &      \\ \hline
    \testref{t:m1} &    &    &    &    &      &      & X    &      \\ \hline
    \testref{t:p1} &    &    &    &    &      &      &      & X    \\ \hline
    \end{tabular}
    \end{table}

\wss{Provide a table that shows which test cases are supporting which requirements.}

\section{Unit Test Description} \label{sec:unit_test}

\wss{This section should not be filled in until after the MIS (detailed design document) has been
  completed.}

\wss{Reference your MIS (detailed design document) and explain your overall philosophy for test case
selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially layout your testing strategy here.  That is, you can explain
how tests will be selected for each module.  For instance, your test building approach could be test
cases for each access program, including one test for normal behaviour and as many tests as needed
for edge cases.  Rather than create the details of the input and output here, you could point to the
unit testing code.  For this to work, you code needs to be well-documented, with meaningful names
for all of the tests.}

\subsection{Unit Testing Scope}

This will be updated when the MG and MIS are complete.

\wss{What modules are outside of the scope.  If there are modules that are developed by someone
  else, then you would say here if you aren't planning on verifying them.  There may also be modules
  that are part of your software, but have a lower priority for verification than others.  If this
  is the case, explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

This will be updated when the MG and MIS are complete.

\wss{Most of the verification will be through automated unit testing.  If appropriate specific
  modules can be verified by a non-testing based technique.  That can also be documented in this
  section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module. References to the
  MIS would be good.  You will want tests from a black box perspective and from a white box
  perspective.  Explain to the reader how the tests were selected.}

\begin{enumerate}

\item[\refstepcounter{utestnum} \UTthetestnum:] \textbf{Inputs: Invalid time} \\
                
    \textbf{Initial State:} ...
                
    \textbf{Input:} ...
            
    \textbf{Output:} ...

    \textbf{Test Case Derivation:} ...
            
    \textbf{How test will be performed:} ...

\item[\refstepcounter{utestnum} \UTthetestnum:] \textbf{Inputs: Invalid mag ref} \\
                
    \textbf{Initial State:} ...
                
    \textbf{Input:} ...
            
    \textbf{Output:} ...

    \textbf{Test Case Derivation:} ...
            
    \textbf{How test will be performed:} ...

\item[\refstepcounter{utestnum} \UTthetestnum:] \textbf{Inputs: Invalid gamma} \\
                
    \textbf{Initial State:} ...
                
    \textbf{Input:} ...
            
    \textbf{Output:} ...

    \textbf{Test Case Derivation:} ...
            
    \textbf{How test will be performed:} ...

\item[\refstepcounter{utestnum} \UTthetestnum:] \textbf{Inputs: Invalid output type} \\
                
    \textbf{Initial State:} ...
                
    \textbf{Input:} ...
            
    \textbf{Output:} ...

    \textbf{Test Case Derivation:} ...
            
    \textbf{How test will be performed:} ...

\item[\refstepcounter{utestnum} \UTthetestnum:] \textbf{Inputs: Valid} \\
                
    \textbf{Initial State:} ...
                
    \textbf{Input:} ...
            
    \textbf{Output:} ...

    \textbf{Test Case Derivation:} ...
            
    \textbf{How test will be performed:} ...
        
\end{enumerate}

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for performance, those test cases
  can go here.  In some projects, planning for nonfunctional tests of units will not be that
  relevant.}

\wss{These tests may involve collecting performance data from previously mentioned functional
  tests.}

This will be updated when the MG and MIS are complete.

\subsubsection{Module TBD}
		
\begin{enumerate}

\item[\refstepcounter{utestnum} \UTthetestnum:] \textbf{test-id1} \\

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\end{enumerate}


\subsection{Traceability Between Test Cases and Modules}

This will be updated when the MG and MIS are complete.

\wss{Provide evidence that all of the modules have been considered.}
				
\newpage
\bibliographystyle{plainnat}

\bibliography{../../refs/References, ../../refs/bib}

\newpage

\section{Appendix}

% This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS. Their values are defined in this
section for easy maintenance.

\begin{table}[h]
    \centering
    \begin{tabular}{|l|l|}
    \hline
    \textbf{Param} & \textbf{Value} \\ \hline
    $\epsilon$     &      5 \%          \\ \hline
    \end{tabular}
    \end{table}

% \subsection{Understandability Survey Questions?}

% \wss{This is a section that would be appropriate for some projects.}

% \newpage{} \section*{Appendix --- Reflection}

% The information in this section will be used to evaluate the team members on the graduate
% attribute of Lifelong Learning.  Please answer the following questions:

\subsection{IMU Test Data} \label{sec:imu_data}

\begin{landscape}
\begin{table}[]
\caption{Sample of ground truth quaternion.}
\label{tab:gt_label}
% \small
\centering
\begin{tabular}{|l||l|l|l|l|} \hline
$t$ & $q_w$ & $q_x$ & $q_y$ & $q_z$ \\ \hline
\hline
0&0.999926 & 0.001208 & -0.002004 & -0.011972 \\ \hline
1&0.999923 & 0.001884 & -0.002375 & -0.012036 \\\hline
2&0.999922 & 0.002302 & -0.002478 & -0.012057 \\\hline
3&0.999925 & 0.002024 & -0.001862 & -0.011960 \\\hline
4&0.999927 & 0.001747 & -0.001245 & -0.011863 \\\hline
5&0.999927 & 0.001672 & -0.001145 & -0.011925 \\\hline
6&0.999926 & 0.001621 & -0.001108 & -0.012006 \\\hline
7&0.999926 & 0.001454 & -0.001265 & -0.012036 \\\hline
8&0.999926 & 0.001176 & -0.001609 & -0.012017 \\\hline
9&0.999926 & 0.001012 & -0.001949 & -0.012006 \\ \hline
\end{tabular}
\end{table}

\begin{table}[]
    \centering
    \caption{Sample of sequential sensor measurements}
    % \footnotesize
    \label{tab:gt_data}
    \begin{tabular}{|l||l|l|l|l|l|l|l|l|l|}
    \hline
    {$t$} & {$\text{acc}_x$} & {$\text{acc}_y$} & {$\text{acc}_z$} &
    {$\text{gyro}_x$} & {$\text{gyro}_y$} & {$\text{gyro}_z$} &
    {$\text{mag}_x$} & {$\text{mag}_y$} & {$\text{mag}_z$} \\ \hline \hline
    0 & 0.090941 & -0.031273 & 9.759028 & 0.005327 & -0.000000 & -0.004260 & 0.554144  & 14.948225 &
    -41.506577 \\ \hline
    1 & 0.130181 & -0.001843 & 9.793363 & 0.005327 & 0.001065  & -0.008522 & 0.035031  & 15.327949 &
    -41.656347 \\ \hline
    2 & 0.072302 & -0.006748 & 9.864976 & 0.003196 & 0.001065  & -0.007458 & -0.484083 & 15.707673 &
    -41.806118 \\ \hline
    3 & 0.062492 & 0.002081  & 9.855166 & 0.000000 & 0.002131  & -0.005327 & 0.183349  & 15.707673 &
    -40.982379 \\ \hline
    4 & 0.090941 & 0.069770  & 9.754123 & 0.005327 & 0.003196  & -0.001065 & 0.850781  & 15.707673 &
    -40.158640 \\ \hline
    5 & 0.077207 & 0.021701  & 9.735484 & 0.003196 & 0.002131  & -0.005327 & 0.850781  & 15.707673 &
    -40.158640 \\ \hline
    6 & 0.072302 & -0.112696 & 9.812002 & 0.003196 & 0.001065  & -0.004260 & 0.850781  & 15.707673 &
    -40.158640 \\ \hline
    7 & 0.115466 & 0.069770  & 9.893425 & 0.004260 & 0.001065  & -0.002131 & 0.183349  & 15.555783 &
    -40.308411 \\ \hline
    8 & 0.052682 & 0.021701  & 9.807097 & 0.001065 & 0.004260  & -0.003196 & -0.484083 & 15.403894 &
    -40.458182 \\ \hline
    9 & 0.029138 & 0.006986  & 9.816907 & 0.004260 & 0.001065  & -0.005327 & -0.261606 & 15.631728 &
    -41.057264 \\ \hline
    \end{tabular}
    \end{table}
\end{landscape}


\end{document}